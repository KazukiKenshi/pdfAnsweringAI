{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93aedd0-aaec-4e47-a905-758d04a90a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1982ad20-c505-41c5-b8d8-e16198635c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdedfa34-2d71-4ed8-ad7f-958388b41bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './models/distilbert_qa_finetuned.pt'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdca6c05-c3ed-4877-971c-3ef50ad54f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5721ddd2-45b4-485b-9221-a21dad1a4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0865d3df-b668-4d41-9e7d-3aea73655882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "eval_data_path = 'datasets/squad_valid.parquet'  \n",
    "eval_df = pd.read_parquet(eval_data_path)\n",
    "eval_data = eval_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6331d9b3-7a44-4415-91bf-c3b6067a24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands.\n",
      "What was the naval base called?\n",
      "['Dyrrachium' 'Dyrrachium' 'Dyrrachium']\n"
     ]
    }
   ],
   "source": [
    "data = eval_df.iloc[1100]\n",
    "print(data['context'])\n",
    "print(data['question'])\n",
    "print((data['answers']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6c476e-b96d-4080-86d8-c9b8404a9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, eval_data, device):\n",
    "    em_total = 0\n",
    "    f1_total = 0\n",
    "    n = len(eval_data)\n",
    "\n",
    "    print(eval_data)\n",
    "    \n",
    "    for item in eval_data:\n",
    "        question = item['question']\n",
    "        context = item['context']\n",
    "        true_answers = item['answers']['text']  # Assuming true_answers is a list of strings\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to GPU\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Move model to GPU\n",
    "            model = model.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        \n",
    "        # Move logits to CPU for decoding\n",
    "        start_scores = start_scores.cpu()\n",
    "        end_scores = end_scores.cpu()\n",
    "        \n",
    "        start_idx = torch.argmax(start_scores)\n",
    "        end_idx = torch.argmax(end_scores)\n",
    "        \n",
    "        predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "        \n",
    "        # Initialize flags for EM and F1 calculation for each QA pair\n",
    "        em_matched = False\n",
    "        f1_matched = False\n",
    "        \n",
    "        # Check predicted answer against each true answer\n",
    "        for true_answer in true_answers:\n",
    "            true_answer = true_answer.lower()  # Convert true answer to lowercase\n",
    "            \n",
    "            # Calculate Exact Match (EM)\n",
    "            if not em_matched:\n",
    "                em = 1 if predicted_answer.strip().lower() == true_answer.strip().lower() else 0\n",
    "                if em == 1:\n",
    "                    em_matched = True\n",
    "                    em_total += 1  # Count as 1 if any true answer matches\n",
    "            \n",
    "            # Calculate F1 Score\n",
    "            predicted_tokens = set(predicted_answer.lower().split())  # Convert predicted answer tokens to lowercase set\n",
    "            true_tokens = set(true_answer.lower().split())  # Convert true answer tokens to lowercase set\n",
    "            \n",
    "            if len(predicted_tokens) == 0 or len(true_tokens) == 0:\n",
    "                continue  # Skip empty predictions or true answers\n",
    "            \n",
    "            common_tokens = predicted_tokens.intersection(true_tokens)\n",
    "            \n",
    "            if len(common_tokens) == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                precision = len(common_tokens) / len(predicted_tokens)\n",
    "                recall = len(common_tokens) / len(true_tokens)\n",
    "                if precision + recall == 0:\n",
    "                    f1 = 0\n",
    "                else:\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                f1_total += f1\n",
    "                if f1 > 0:\n",
    "                    f1_matched = True\n",
    "        \n",
    "        # If no match found for EM, count as 0\n",
    "        if not em_matched:\n",
    "            em_total += 0\n",
    "        \n",
    "        # If no match found for F1, count as 0\n",
    "        if not f1_matched:\n",
    "            f1_total += 0\n",
    "    \n",
    "    em_score = em_total / n\n",
    "    f1_score = f1_total / n\n",
    "    \n",
    "    return em_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "babbeac6-9b7d-4f20-818d-990a0b8a3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em_score, f1_score = evaluate_model(model, tokenizer, eval_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae758f9-904f-41ec-8ff3-98af6fbef322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 0.62\n",
      "F1 Score: 2.17\n"
     ]
    }
   ],
   "source": [
    "print(f'Exact Match (EM): {em_score:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85123db9-bf6c-4404-b557-7568a8525b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43b1d82-7437-45ed-b4dd-10a1e4d5dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_answer(model, tokenizer, context, question, device):\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop('offset_mapping').cpu().numpy()[0]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores)\n",
    "    actual_start_idx = offset_mapping[start_idx][0]\n",
    "    actual_end_idx = offset_mapping[end_idx][1]\n",
    "    \n",
    "    \n",
    "    predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "\n",
    "    return predicted_answer, actual_start_idx, actual_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69080015-3d56-4b77-a9ee-3a16cd647598",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Marshall Bruce Mathers III was born on October 17, 1972, in St. Joseph, Missouri, the only child of Marshall Bruce Mathers Jr. and Deborah Rae \"Debbie\" (née Nelson).[12][13] His mother nearly died during her 73-hour labor with him.[14] Eminem's parents were in a band called Daddy Warbucks, playing in Ramada Inns along the Dakotas–Montana border before they separated. His father abandoned his family when Eminem was a year and a half old, and Eminem was raised only by his mother, Debbie, in poverty.[12] His mother later had a son named Nathan \"Nate\" Kane Samara.[15] By age twelve, Eminem and his mother had moved several times and lived in several towns and cities in Missouri (including St. Joseph, Savannah, and Kansas City) before settling in Warren, Michigan, a suburb of Detroit.[16] Eminem frequently fought with his mother, whom a social worker described as having a \"very suspicious, almost paranoid personality\".[17] He wrote letters to his father, but Debbie said that they all came back marked \"return to sender\".[18]\n",
    "When he was a child, a bully named D'Angelo Bailey severely injured Eminem's head in an assault,[19] an incident which Eminem later recounted (with comic exaggeration) on the song \"Brain Damage\". Debbie filed a lawsuit against the public school for this in 1982. The suit was dismissed the following year by a Macomb County, Michigan, judge, who said the schools were immune from lawsuits.[14] For much of his youth, Eminem and his mother lived in a working-class, primarily black, Detroit neighborhood. He and Debbie were one of three white households on their block, and Eminem was beaten several times by black youths.[18]\n",
    "Eminem was interested in storytelling, aspiring to be a comic book artist before discovering hip hop.[20] He heard his first rap song (\"Reckless\", featuring Ice-T) on the Breakin' soundtrack, a gift from Debbie's half-brother, Ronnie Polkingharn. His uncle was close to the boy and later became a musical mentor to him. When Polkingharn committed suicide in 1991, Eminem stopped speaking publicly for days and did not attend his funeral.[18][21]\n",
    "At age 14, Eminem began rapping with high-school friend Mike Ruby; they adopted the names \"Manix\" and \"M&M\", the latter evolving into \"Eminem\".[21][14] Eminem snuck into neighboring Osborn High School with friend and fellow rapper Proof for lunchroom freestyle rap battles.[22] On Saturdays, they attended open mic contests at the Hip-Hop Shop on West 7 Mile Road, considered \"ground zero\" for the Detroit rap scene.[18] Struggling to succeed in a predominantly black industry, Eminem was appreciated by underground hip hop audiences.[21][23][24] When he wrote verses, he wanted most of the words to rhyme; he wrote long words or phrases on paper and, underneath, worked on rhymes for each syllable. Although the words often made little sense, the drill helped Eminem practice sounds and rhymes.[25]\n",
    "In 1987, Debbie allowed runaway Kimberly Anne \"Kim\" Scott to stay at their home. Several years later, Eminem began an on-and-off relationship with Scott.[14] After spending three years in ninth grade due to truancy and poor grades,[26] he dropped out of Lincoln High School at age 17. Although interested in English, Eminem never explored literature (preferring comic books) and he disliked math and social studies.[25] Eminem worked at several jobs to help his mother pay the bills. One of the jobs he had was with Little Caesar's Pizza in Warren.[27] He later said she often threw him out of the house anyway, often after taking most of his paycheck. When she left to play bingo, he would blast the stereo and write songs.[18]\n",
    "\"\"\"\n",
    "\n",
    "context = \"Eminem's first rap was reckless. His true name is Marshall Bruce Mathers III. He is a legend in rapping. A dog is called kutta in hindi. Cow is a sacred animal in hinduism\"\n",
    "question = \"What is dog called in hindi\"\n",
    "model.to('cpu')\n",
    "answer, start,end = predicted_answer(model, tokenizer, context, question, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0523dc97-03de-4bfd-8594-2f507c038746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reckless\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fa4fd9-fa7f-40cc-b470-ef927f83c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"\"\n",
    "i = 1\n",
    "YELLOW = '\\033[33m'\n",
    "for ch in context:\n",
    "    \n",
    "    if(i==start):\n",
    "        ch = \"#\"\n",
    "\n",
    "    if(i==end):\n",
    "        ch = \"#\"\n",
    "\n",
    "    out += ch\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8ea73d7-898e-4dbf-a2ad-7999a91f6435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMarshall Bruce Mathers III was born on October 17, 1972, in St. Joseph, Missouri, the only child of Marshall Bruce Mathers Jr. and Deborah Rae \"Debbie\" (née Nelson).[12][13] His mother nearly died during her 73-hour labor with him.[14] Eminem\\'s parents were in a band called Daddy Warbucks, playing in Ramada Inns along the Dakotas–Montana border before they separated. His father abandoned his family when Eminem was a year and a half old, and Eminem was raised only by his mother, Debbie, in poverty.[12] His mother later had a son named Nathan \"Nate\" Kane Samara.[15] By age twelve, Eminem and his mother had moved several times and lived in several towns and cities in Missouri (including St. Joseph, Savannah, and Kansas City) before settling in Warren, Michigan, a suburb of Detroit.[16] Eminem frequently fought with his mother, whom a social worker described as having a \"very suspicious, almost paranoid personality\".[17] He wrote letters to his father, but Debbie said that they all came back marked \"return to sender\".[18]\\nWhen he was a child, a bully named#D\\'Angelo Baile# severely injured Eminem\\'s head in an assault,[19] an incident which Eminem later recounted (with comic exaggeration) on the song \"Brain Damage\". Debbie filed a lawsuit against the public school for this in 1982. The suit was dismissed the following year by a Macomb County, Michigan, judge, who said the schools were immune from lawsuits.[14] For much of his youth, Eminem and his mother lived in a working-class, primarily black, Detroit neighborhood. He and Debbie were one of three white households on their block, and Eminem was beaten several times by black youths.[18]\\nEminem was interested in storytelling, aspiring to be a comic book artist before discovering hip hop.[20] He heard his first rap song (\"Reckless\", featuring Ice-T) on the Breakin\\' soundtrack, a gift from Debbie\\'s half-brother, Ronnie Polkingharn. His uncle was close to the boy and later became a musical mentor to him. When Polkingharn committed suicide in 1991, Eminem stopped speaking publicly for days and did not attend his funeral.[18][21]\\nAt age 14, Eminem began rapping with high-school friend Mike Ruby; they adopted the names \"Manix\" and \"M&M\", the latter evolving into \"Eminem\".[21][14] Eminem snuck into neighboring Osborn High School with friend and fellow rapper Proof for lunchroom freestyle rap battles.[22] On Saturdays, they attended open mic contests at the Hip-Hop Shop on West 7 Mile Road, considered \"ground zero\" for the Detroit rap scene.[18] Struggling to succeed in a predominantly black industry, Eminem was appreciated by underground hip hop audiences.[21][23][24] When he wrote verses, he wanted most of the words to rhyme; he wrote long words or phrases on paper and, underneath, worked on rhymes for each syllable. Although the words often made little sense, the drill helped Eminem practice sounds and rhymes.[25]\\nIn 1987, Debbie allowed runaway Kimberly Anne \"Kim\" Scott to stay at their home. Several years later, Eminem began an on-and-off relationship with Scott.[14] After spending three years in ninth grade due to truancy and poor grades,[26] he dropped out of Lincoln High School at age 17. Although interested in English, Eminem never explored literature (preferring comic books) and he disliked math and social studies.[25] Eminem worked at several jobs to help his mother pay the bills. One of the jobs he had was with Little Caesar\\'s Pizza in Warren.[27] He later said she often threw him out of the house anyway, often after taking most of his paycheck. When she left to play bingo, he would blast the stereo and write songs.[18]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52bc83fb-cc65-452e-ba66-4994cf51776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = './RMall.pdf'\n",
    "\n",
    "# Open the PDF file\n",
    "with open(pdf_path, 'rb') as pdf_file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    # Get the number of pages in the PDF\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    \n",
    "    # Initialize a string to hold the text content\n",
    "    text_content = ''\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text_content += page.extract_text()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a73b6d0-cb92-49a2-9662-5208493cddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Structured Analysis?\"\n",
    "context = text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "289d798c-03dc-4c19-b029-96ae97a2fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, start,end = predicted_answer(model, tokenizer, context, question, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d3279453-3058-4c68-a743-b704fd468622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fundamentals\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ed95435-7233-44fa-84ff-6b1443710304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start :  171\n",
      "end :  69\n"
     ]
    }
   ],
   "source": [
    "print(\"start : \",start)\n",
    "print(\"end : \",end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e72ef14b-4d7b-4c0c-a1aa-628d3fe5520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hitesh\n",
      "[nltk_data]     Dhiman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hitesh Dhiman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "63dd789a-7778-4e66-8e93-0aebfc5099a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5977f79-bcd1-429d-a7ec-94739ee6758e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Either 'hosts' or 'cloud_id' must be specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Initialize Elasticsearch client\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m es \u001b[38;5;241m=\u001b[39m Elasticsearch()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex_document_segments\u001b[39m(document_text):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Split document into segments (e.g., paragraphs, sections)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     segments \u001b[38;5;241m=\u001b[39m tokenize_into_sentences(document_text)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\notebooks\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:196\u001b[0m, in \u001b[0;36mElasticsearch.__init__\u001b[1;34m(self, hosts, cloud_id, api_key, basic_auth, bearer_auth, opaque_id, headers, connections_per_node, http_compress, verify_certs, ca_certs, client_cert, client_key, ssl_assert_hostname, ssl_assert_fingerprint, ssl_version, ssl_context, ssl_show_warn, transport_class, request_timeout, node_class, node_pool_class, randomize_nodes_in_pool, node_selector_class, dead_node_backoff_factor, max_dead_node_backoff, serializer, serializers, default_mimetype, max_retries, retry_on_status, retry_on_timeout, sniff_on_start, sniff_before_requests, sniff_on_node_failure, sniff_timeout, min_delay_between_sniffing, sniffed_node_callback, meta_header, timeout, randomize_hosts, host_info_callback, sniffer_timeout, sniff_on_connection_fail, http_auth, maxsize, _transport)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    135\u001b[0m     hosts: t\u001b[38;5;241m.\u001b[39mOptional[_TYPE_HOSTS] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     _transport: t\u001b[38;5;241m.\u001b[39mOptional[Transport] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hosts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cloud_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhosts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEFAULT:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m request_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEFAULT:\n",
      "\u001b[1;31mValueError\u001b[0m: Either 'hosts' or 'cloud_id' must be specified"
     ]
    }
   ],
   "source": [
    "# Example workflow to find related sections in a PDF\n",
    "\n",
    "# 1. Extract text from PDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(reader.numPages):\n",
    "            page = reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "        return text\n",
    "\n",
    "# 2. Preprocess and tokenize text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Perform text cleaning, normalization, etc.\n",
    "    # Example: Remove special characters, excess whitespace, etc.\n",
    "    cleaned_text = text.replace('\\n', ' ')\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize_into_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# 3. Semantic analysis (e.g., keyword extraction, NER, topic modeling)\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    # Extract keywords using POS tagging\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    keywords = [token for token, pos in tagged_tokens if pos.startswith('NN') or pos.startswith('VB')]\n",
    "    return keywords\n",
    "\n",
    "def extract_named_entities(sentence):\n",
    "    # Extract named entities\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    return named_entities\n",
    "\n",
    "# 4. Indexing or database setup (using Elasticsearch for example)\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch()\n",
    "\n",
    "def index_document_segments(document_text):\n",
    "    # Split document into segments (e.g., paragraphs, sections)\n",
    "    segments = tokenize_into_sentences(document_text)\n",
    "    \n",
    "    # Index each segment into Elasticsearch\n",
    "    for idx, segment in enumerate(segments):\n",
    "        es.index(index='pdf_segments', id=idx, body={'text': segment})\n",
    "        \n",
    "# 5. Query processing and retrieval\n",
    "def query_related_sections(query):\n",
    "    # Preprocess query\n",
    "    processed_query = preprocess_text(query)\n",
    "    \n",
    "    # Search relevant segments using Elasticsearch\n",
    "    search_results = es.search(index='pdf_segments', body={'query': {'match': {'text': processed_query}}})\n",
    "    \n",
    "    # Retrieve relevant segments\n",
    "    relevant_segments = [hit['_source']['text'] for hit in search_results['hits']['hits']]\n",
    "    \n",
    "    return relevant_segments\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa20ab8-5ee5-42f0-9fe8-394194919a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'path/to/your/pdf/document.pdf'\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "index_document_segments(document_text)\n",
    "\n",
    "query = 'Eminem childhood'\n",
    "related_sections = query_related_sections(query)\n",
    "for section in related_sections:\n",
    "    print(section)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-kernel",
   "language": "python",
   "name": "notebooks-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
