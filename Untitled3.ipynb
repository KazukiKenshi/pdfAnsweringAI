{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93aedd0-aaec-4e47-a905-758d04a90a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1982ad20-c505-41c5-b8d8-e16198635c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdedfa34-2d71-4ed8-ad7f-958388b41bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './models/distilbert_qa_finetuned.pt'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdca6c05-c3ed-4877-971c-3ef50ad54f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5721ddd2-45b4-485b-9221-a21dad1a4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0865d3df-b668-4d41-9e7d-3aea73655882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "eval_data_path = 'datasets/squad_valid.parquet'  \n",
    "eval_df = pd.read_parquet(eval_data_path)\n",
    "eval_data = eval_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6331d9b3-7a44-4415-91bf-c3b6067a24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands.\n",
      "What was the naval base called?\n",
      "['Dyrrachium' 'Dyrrachium' 'Dyrrachium']\n"
     ]
    }
   ],
   "source": [
    "data = eval_df.iloc[1100]\n",
    "print(data['context'])\n",
    "print(data['question'])\n",
    "print((data['answers']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6c476e-b96d-4080-86d8-c9b8404a9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, eval_data, device):\n",
    "    em_total = 0\n",
    "    f1_total = 0\n",
    "    n = len(eval_data)\n",
    "\n",
    "    print(eval_data)\n",
    "    \n",
    "    for item in eval_data:\n",
    "        question = item['question']\n",
    "        context = item['context']\n",
    "        true_answers = item['answers']['text']  # Assuming true_answers is a list of strings\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to GPU\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Move model to GPU\n",
    "            model = model.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        \n",
    "        # Move logits to CPU for decoding\n",
    "        start_scores = start_scores.cpu()\n",
    "        end_scores = end_scores.cpu()\n",
    "        \n",
    "        start_idx = torch.argmax(start_scores)\n",
    "        end_idx = torch.argmax(end_scores)\n",
    "        \n",
    "        predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "        \n",
    "        # Initialize flags for EM and F1 calculation for each QA pair\n",
    "        em_matched = False\n",
    "        f1_matched = False\n",
    "        \n",
    "        # Check predicted answer against each true answer\n",
    "        for true_answer in true_answers:\n",
    "            true_answer = true_answer.lower()  # Convert true answer to lowercase\n",
    "            \n",
    "            # Calculate Exact Match (EM)\n",
    "            if not em_matched:\n",
    "                em = 1 if predicted_answer.strip().lower() == true_answer.strip().lower() else 0\n",
    "                if em == 1:\n",
    "                    em_matched = True\n",
    "                    em_total += 1  # Count as 1 if any true answer matches\n",
    "            \n",
    "            # Calculate F1 Score\n",
    "            predicted_tokens = set(predicted_answer.lower().split())  # Convert predicted answer tokens to lowercase set\n",
    "            true_tokens = set(true_answer.lower().split())  # Convert true answer tokens to lowercase set\n",
    "            \n",
    "            if len(predicted_tokens) == 0 or len(true_tokens) == 0:\n",
    "                continue  # Skip empty predictions or true answers\n",
    "            \n",
    "            common_tokens = predicted_tokens.intersection(true_tokens)\n",
    "            \n",
    "            if len(common_tokens) == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                precision = len(common_tokens) / len(predicted_tokens)\n",
    "                recall = len(common_tokens) / len(true_tokens)\n",
    "                if precision + recall == 0:\n",
    "                    f1 = 0\n",
    "                else:\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                f1_total += f1\n",
    "                if f1 > 0:\n",
    "                    f1_matched = True\n",
    "        \n",
    "        # If no match found for EM, count as 0\n",
    "        if not em_matched:\n",
    "            em_total += 0\n",
    "        \n",
    "        # If no match found for F1, count as 0\n",
    "        if not f1_matched:\n",
    "            f1_total += 0\n",
    "    \n",
    "    em_score = em_total / n\n",
    "    f1_score = f1_total / n\n",
    "    \n",
    "    return em_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "babbeac6-9b7d-4f20-818d-990a0b8a3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em_score, f1_score = evaluate_model(model, tokenizer, eval_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae758f9-904f-41ec-8ff3-98af6fbef322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 0.62\n",
      "F1 Score: 2.17\n"
     ]
    }
   ],
   "source": [
    "print(f'Exact Match (EM): {em_score:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85123db9-bf6c-4404-b557-7568a8525b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43b1d82-7437-45ed-b4dd-10a1e4d5dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_answer(model, tokenizer, context, question, device):\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop('offset_mapping').cpu().numpy()[0]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores)\n",
    "    actual_start_idx = offset_mapping[start_idx][0]\n",
    "    actual_end_idx = offset_mapping[end_idx][1]\n",
    "    \n",
    "    \n",
    "    predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "\n",
    "    return predicted_answer, actual_start_idx, actual_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69080015-3d56-4b77-a9ee-3a16cd647598",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Marshall Bruce Mathers III was born on October 17, 1972, in St. Joseph, Missouri, the only child of Marshall Bruce Mathers Jr. and Deborah Rae \"Debbie\" (née Nelson).[12][13] His mother nearly died during her 73-hour labor with him.[14] Eminem's parents were in a band called Daddy Warbucks, playing in Ramada Inns along the Dakotas–Montana border before they separated. His father abandoned his family when Eminem was a year and a half old, and Eminem was raised only by his mother, Debbie, in poverty.[12] His mother later had a son named Nathan \"Nate\" Kane Samara.[15] By age twelve, Eminem and his mother had moved several times and lived in several towns and cities in Missouri (including St. Joseph, Savannah, and Kansas City) before settling in Warren, Michigan, a suburb of Detroit.[16] Eminem frequently fought with his mother, whom a social worker described as having a \"very suspicious, almost paranoid personality\".[17] He wrote letters to his father, but Debbie said that they all came back marked \"return to sender\".[18]\n",
    "When he was a child, a bully named D'Angelo Bailey severely injured Eminem's head in an assault,[19] an incident which Eminem later recounted (with comic exaggeration) on the song \"Brain Damage\". Debbie filed a lawsuit against the public school for this in 1982. The suit was dismissed the following year by a Macomb County, Michigan, judge, who said the schools were immune from lawsuits.[14] For much of his youth, Eminem and his mother lived in a working-class, primarily black, Detroit neighborhood. He and Debbie were one of three white households on their block, and Eminem was beaten several times by black youths.[18]\n",
    "Eminem was interested in storytelling, aspiring to be a comic book artist before discovering hip hop.[20] He heard his first rap song (\"Reckless\", featuring Ice-T) on the Breakin' soundtrack, a gift from Debbie's half-brother, Ronnie Polkingharn. His uncle was close to the boy and later became a musical mentor to him. When Polkingharn committed suicide in 1991, Eminem stopped speaking publicly for days and did not attend his funeral.[18][21]\n",
    "At age 14, Eminem began rapping with high-school friend Mike Ruby; they adopted the names \"Manix\" and \"M&M\", the latter evolving into \"Eminem\".[21][14] Eminem snuck into neighboring Osborn High School with friend and fellow rapper Proof for lunchroom freestyle rap battles.[22] On Saturdays, they attended open mic contests at the Hip-Hop Shop on West 7 Mile Road, considered \"ground zero\" for the Detroit rap scene.[18] Struggling to succeed in a predominantly black industry, Eminem was appreciated by underground hip hop audiences.[21][23][24] When he wrote verses, he wanted most of the words to rhyme; he wrote long words or phrases on paper and, underneath, worked on rhymes for each syllable. Although the words often made little sense, the drill helped Eminem practice sounds and rhymes.[25]\n",
    "In 1987, Debbie allowed runaway Kimberly Anne \"Kim\" Scott to stay at their home. Several years later, Eminem began an on-and-off relationship with Scott.[14] After spending three years in ninth grade due to truancy and poor grades,[26] he dropped out of Lincoln High School at age 17. Although interested in English, Eminem never explored literature (preferring comic books) and he disliked math and social studies.[25] Eminem worked at several jobs to help his mother pay the bills. One of the jobs he had was with Little Caesar's Pizza in Warren.[27] He later said she often threw him out of the house anyway, often after taking most of his paycheck. When she left to play bingo, he would blast the stereo and write songs.[18]\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who injured eminem in his childhood\"\n",
    "model.to('cpu')\n",
    "answer, start,end = predicted_answer(model, tokenizer, context, question, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0523dc97-03de-4bfd-8594-2f507c038746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d'angelo bailey\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-kernel",
   "language": "python",
   "name": "notebooks-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
